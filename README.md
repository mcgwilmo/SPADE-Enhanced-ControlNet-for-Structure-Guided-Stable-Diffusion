Recent advances in text-to-image synthesis through diffusion models have achieved remarkable results, yet maintaining precise spatial fidelity to structural conditions still remains a persistent challenge. Therefore, building upon the ControlNet framework for Stable Diffusion, we propose a novel integration of SPADE (Spatially-Adaptive Normalization) to enhance structural conditioning while preserving the model’s generative flexibility. Our novel method addresses the inherent tradeoff between adherence to semantic layouts and output realism by introducing dual-path modulation: one branch processes user-specified segmentation maps through SPADE’s spatially adaptive normalization, while the other maintains ControlNet’s original conditioning mechanisms[1, 2, 3]. To achieve this then, we finetune a pretrained ControlNet model with our SPADE-enhanced architecture on the ADE20K dataset’s dense segmentation annotations, eliminating the computational burden of training from scratch. Quantitative evaluation demonstrates significant improvements in structural coherence, with SSIM scores increasing by 23.2 percent over baseline ControlNet while maintaining competitive FID scores (268 vs. 294). On the other hand, qualitative analysis reveals that SPADEControlNet is better at maintaing perspective consistency and color trueness. It is hence obvious that by bridging the strengths of SPADE’s spatial awareness with ControlNet’s stable diffusion backbone, our work advances controllable image synthesis for applications requiring precise layout adherence, from architectural visualization to virtual scene generation.
